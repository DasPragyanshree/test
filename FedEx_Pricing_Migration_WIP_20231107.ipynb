{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289e628f",
   "metadata": {},
   "source": [
    "## Reading files and creating dataframes to store all relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d492c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import numpy as np\n",
    "from pandasql import sqldf\n",
    "import pyxlsb\n",
    "import xlwings as xw\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0234f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARDCODING the SPT file path\n",
    "#spt_tool_path = 'C:\\\\FedEx\\\\FDX SPT - 2024 SAS v4.3 - ES Export_EUR v4 - Accenture MasterCopy v 0.1.xlsb'\n",
    "\n",
    "spt_tool_path = 'C:\\\\FedEx\\\\FDX SPT - 2024 SAS v4.3 - FR Export_EUR_update_v2.xlsb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0c270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read tables from a single sheet with 5 given parameters\n",
    "\n",
    "def read_tables(file_path_and_name,\n",
    "                sheet_name, \n",
    "                number_of_rows_above_the_table, \n",
    "                number_of_rows_in_the_table_excluding_headers, \n",
    "                excel_column_range): # feeding the parameters required into the function definition\n",
    "    \n",
    "    df = pd.read_excel(file_path_and_name, \n",
    "                       engine='pyxlsb', \n",
    "                       sheet_name = sheet_name, \n",
    "                       skiprows = number_of_rows_above_the_table,  \n",
    "                       nrows= number_of_rows_in_the_table_excluding_headers, \n",
    "                       usecols = excel_column_range,\n",
    "                      parse_dates=[0]) # reading a table from an excel sheet\n",
    "    \n",
    "    return df # returning the table in pandas dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faee1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tables listed in the Param sheet of the SPT tool\n",
    "\n",
    "def spt_tool_tables(spt_tool_path,parameter_sheet_name):\n",
    "    \n",
    "    param = pd.read_excel(spt_tool_path, \n",
    "                          engine='pyxlsb', \n",
    "                          sheet_name = parameter_sheet_name) # reading the Param sheet from the SPT tool\n",
    "    \n",
    "    table_dict = {} # creating empty dictionary to store the tables that will be read\n",
    "    sheetname=set() #creating empty set to store sheetname\n",
    "    \n",
    "    for i in range(len(param)):\n",
    "        table_i = read_tables(spt_tool_path,\n",
    "                         param.iat[i,1], # this parameter is the sheet name\n",
    "                         param.iat[i,2], # this parameter is the #rows above the table\n",
    "                         param.iat[i,3], # this parameter is the #rows in the table, excluding headers\n",
    "                         param.iat[i,4]) # this parameter is the excel column range\n",
    "        \n",
    "        table_dict[str(param.iat[i,0])] = table_i # creating the key:value pair for table name: table\n",
    "        sheetname.add(str(param.iat[i,1])) # Store all sheetname in set\n",
    "\n",
    "    return table_dict,sheetname # returning the table dictionary and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b2e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARDCODING the parameter sheet name\n",
    "parameter_sheet_name = 'Param'\n",
    "all_tables,all_sheetname = spt_tool_tables(spt_tool_path,parameter_sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8b871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['master_data',\n",
       " 'mark_up_premium_products',\n",
       " 'domestic',\n",
       " 'options',\n",
       " 'collection_and_delivery',\n",
       " 'covid_surcharge',\n",
       " 'clearance_fee',\n",
       " 'rec_pays_fee',\n",
       " 'ess',\n",
       " 'ooa',\n",
       " 'special_handling',\n",
       " 'ccf',\n",
       " '09deurope',\n",
       " 'domestic:',\n",
       " 'fuel',\n",
       " 'exchange_rates',\n",
       " 'budget_fx_rates',\n",
       " 'rate_simulator_ip',\n",
       " 'rate_simulator_ie_air',\n",
       " 'rate_simulator_re',\n",
       " 'discount_matrix_ip',\n",
       " 'discount_matrix_ie_air',\n",
       " 'discount_matrix_re']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_tables) # here is the entire list of tables available in the table dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5045fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating separate dataframes for each table\n",
    "master_data = all_tables['master_data']\n",
    "rate_simulator_ip = all_tables['rate_simulator_ip']\n",
    "rate_simulator_ie_air = all_tables['rate_simulator_ie_air']\n",
    "rate_simulator_re = all_tables['rate_simulator_re']\n",
    "discount_matrix_ip = all_tables['discount_matrix_ip']\n",
    "discount_matrix_ie_air = all_tables['discount_matrix_ie_air']\n",
    "discount_matrix_re = all_tables['discount_matrix_re']\n",
    "# created dataframes for only relevant tables as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db63e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Scope</th>\n",
       "      <th>…</th>\n",
       "      <th>GE Number</th>\n",
       "      <th>Inv Country Code</th>\n",
       "      <th>Rate Parent Account Nbr</th>\n",
       "      <th>Rate Parent Account Name</th>\n",
       "      <th>Account Number</th>\n",
       "      <th>Account Name</th>\n",
       "      <th>Shipment Number</th>\n",
       "      <th>...</th>\n",
       "      <th>Error old cons?</th>\n",
       "      <th>Paywgt new vs old - diff too big? (Exi buss only)</th>\n",
       "      <th>Credit note?</th>\n",
       "      <th>Error Frt Rev Old? (Exi buss only)</th>\n",
       "      <th>Error Frt Rev New?</th>\n",
       "      <th>Re-rate Frt Rev?</th>\n",
       "      <th>Error All-in Rev Old?  (Exi buss only)</th>\n",
       "      <th>Error All-in Rev New?</th>\n",
       "      <th>Re-rate All-in Rev?</th>\n",
       "      <th>Freight Rev Old inc RFP + BXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exi</td>\n",
       "      <td>In scope</td>\n",
       "      <td>…</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>94000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>60000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>196283856</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>53.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exi</td>\n",
       "      <td>In scope</td>\n",
       "      <td>…</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>94000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>60000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>196290741</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>81.850001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exi</td>\n",
       "      <td>In scope</td>\n",
       "      <td>…</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>94000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>60000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>196407799</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>9.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exi</td>\n",
       "      <td>In scope</td>\n",
       "      <td>…</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>94000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>60000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>196699876</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>9.360001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exi</td>\n",
       "      <td>In scope</td>\n",
       "      <td>…</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>94000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>60000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>196715273</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>13.790001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70174</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>1036198000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>877135000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>I41997114455</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>28.068248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70175</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>1036198000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>877135000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>I42000928972</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>25.741243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70176</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>1036198000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>877135000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>I42004133665</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>25.757220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70177</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>1036198000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>877135000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>I42009206843</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>28.070411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70178</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1036170000</td>\n",
       "      <td>FR</td>\n",
       "      <td>1036198000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>877135000</td>\n",
       "      <td>Test Account</td>\n",
       "      <td>I42021779578</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Y</td>\n",
       "      <td>28.062680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70179 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type     Scope    …   GE Number Inv Country Code  \\\n",
       "0      Exi  In scope    …  1036170000               FR   \n",
       "1      Exi  In scope    …  1036170000               FR   \n",
       "2      Exi  In scope    …  1036170000               FR   \n",
       "3      Exi  In scope    …  1036170000               FR   \n",
       "4      Exi  In scope    …  1036170000               FR   \n",
       "...    ...       ...  ...         ...              ...   \n",
       "70174  NaN       NaN  NaN  1036170000               FR   \n",
       "70175  NaN       NaN  NaN  1036170000               FR   \n",
       "70176  NaN       NaN  NaN  1036170000               FR   \n",
       "70177  NaN       NaN  NaN  1036170000               FR   \n",
       "70178  NaN       NaN  NaN  1036170000               FR   \n",
       "\n",
       "       Rate Parent Account Nbr Rate Parent Account Name  Account Number  \\\n",
       "0                        94000             Test Account           60000   \n",
       "1                        94000             Test Account           60000   \n",
       "2                        94000             Test Account           60000   \n",
       "3                        94000             Test Account           60000   \n",
       "4                        94000             Test Account           60000   \n",
       "...                        ...                      ...             ...   \n",
       "70174               1036198000             Test Account       877135000   \n",
       "70175               1036198000             Test Account       877135000   \n",
       "70176               1036198000             Test Account       877135000   \n",
       "70177               1036198000             Test Account       877135000   \n",
       "70178               1036198000             Test Account       877135000   \n",
       "\n",
       "       Account Name Shipment Number  ... Error old cons?  \\\n",
       "0      Test Account       196283856  ...           False   \n",
       "1      Test Account       196290741  ...           False   \n",
       "2      Test Account       196407799  ...           False   \n",
       "3      Test Account       196699876  ...           False   \n",
       "4      Test Account       196715273  ...           False   \n",
       "...             ...             ...  ...             ...   \n",
       "70174  Test Account    I41997114455  ...           False   \n",
       "70175  Test Account    I42000928972  ...           False   \n",
       "70176  Test Account    I42004133665  ...           False   \n",
       "70177  Test Account    I42009206843  ...           False   \n",
       "70178  Test Account    I42021779578  ...           False   \n",
       "\n",
       "       Paywgt new vs old - diff too big? (Exi buss only)  Credit note?  \\\n",
       "0                                                  False         False   \n",
       "1                                                  False         False   \n",
       "2                                                  False         False   \n",
       "3                                                  False         False   \n",
       "4                                                  False         False   \n",
       "...                                                  ...           ...   \n",
       "70174                                              False         False   \n",
       "70175                                              False         False   \n",
       "70176                                              False         False   \n",
       "70177                                              False         False   \n",
       "70178                                              False         False   \n",
       "\n",
       "      Error Frt Rev Old? (Exi buss only) Error Frt Rev New?  Re-rate Frt Rev?  \\\n",
       "0                                  False              False                 Y   \n",
       "1                                  False              False                 Y   \n",
       "2                                  False              False                 Y   \n",
       "3                                  False              False                 Y   \n",
       "4                                  False              False                 Y   \n",
       "...                                  ...                ...               ...   \n",
       "70174                              False              False                 Y   \n",
       "70175                              False              False                 Y   \n",
       "70176                              False              False                 Y   \n",
       "70177                              False              False                 Y   \n",
       "70178                              False              False                 Y   \n",
       "\n",
       "      Error All-in Rev Old?  (Exi buss only) Error All-in Rev New?  \\\n",
       "0                                      False                 False   \n",
       "1                                      False                 False   \n",
       "2                                      False                 False   \n",
       "3                                      False                 False   \n",
       "4                                      False                 False   \n",
       "...                                      ...                   ...   \n",
       "70174                                  False                 False   \n",
       "70175                                  False                 False   \n",
       "70176                                  False                 False   \n",
       "70177                                  False                 False   \n",
       "70178                                  False                 False   \n",
       "\n",
       "      Re-rate All-in Rev? Freight Rev Old inc RFP + BXT  \n",
       "0                       Y                     53.600000  \n",
       "1                       Y                     81.850001  \n",
       "2                       Y                      9.360001  \n",
       "3                       Y                      9.360001  \n",
       "4                       Y                     13.790001  \n",
       "...                   ...                           ...  \n",
       "70174                   Y                     28.068248  \n",
       "70175                   Y                     25.741243  \n",
       "70176                   Y                     25.757220  \n",
       "70177                   Y                     28.070411  \n",
       "70178                   Y                     28.062680  \n",
       "\n",
       "[70179 rows x 147 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master_data.groupby(['Sce','Zoning','New Payweightband'])\n",
    "master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5811ca94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m fdx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrt Rev New \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m tnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreight Rev Old inc RFP + BXT\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m pivot1 \u001b[38;5;241m=\u001b[39m \u001b[43mmaster_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZoning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNew Payweightband\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mfdx\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtnt\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      4\u001b[0m pivot1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTNT_vs_FDX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pivot1[fdx] \u001b[38;5;241m-\u001b[39m pivot1[tnt]\n\u001b[0;32m      5\u001b[0m pivot1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (pivot1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTNT_vs_FDX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m/\u001b[39mpivot1[tnt])\u001b[38;5;241m.\u001b[39mreplace(np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:869\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    866\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m    868\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[1;32m--> 869\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:168\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:475\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m     results \u001b[38;5;241m=\u001b[39m {key: colg\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    476\u001b[0m         key: obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    477\u001b[0m     }\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# set the final keys\u001b[39;00m\n\u001b[0;32m    480\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(arg\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:476\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    472\u001b[0m     results \u001b[38;5;241m=\u001b[39m {key: colg\u001b[38;5;241m.\u001b[39magg(how) \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 476\u001b[0m         key: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    477\u001b[0m     }\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# set the final keys\u001b[39;00m\n\u001b[0;32m    480\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(arg\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:265\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, abc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2189\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[1;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;66;03m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;66;03m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;66;03m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2189\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2192\u001b[0m \u001b[43m        \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnpfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_output(result, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1506\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agg_general\u001b[39m(\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1501\u001b[0m     npfunc: Callable,\n\u001b[0;32m   1502\u001b[0m ):\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_selection_context():\n\u001b[0;32m   1505\u001b[0m         \u001b[38;5;66;03m# try a cython aggregation if we can\u001b[39;00m\n\u001b[1;32m-> 1506\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m            \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1512\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1592\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;66;03m# TypeError -> we may have an exception in trying to aggregate\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;66;03m#  continue and exclude the block\u001b[39;00m\n\u001b[1;32m-> 1592\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ser \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_mgr) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m   1595\u001b[0m     warn_dropping_nuisance_columns_deprecated(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), how)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\base.py:199\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[1;34m(self, func, ignore_failures)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mignore_failures : bool, default False\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    Not used; for compatibility with ArrayManager/BlockManager.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\n\u001b[1;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(res))\n\u001b[0;32m    202\u001b[0m mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1578\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_func\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1578\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m         \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m         \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m         \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:939\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m ids, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_info\n\u001b[0;32m    938\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n\u001b[1;32m--> 939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cy_op\u001b[38;5;241m.\u001b[39mcython_operation(\n\u001b[0;32m    940\u001b[0m     values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[0;32m    941\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    942\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    943\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    944\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    946\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:626\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[1;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[0;32m    619\u001b[0m         values,\n\u001b[0;32m    620\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_op_ndim_compat(\n\u001b[0;32m    627\u001b[0m     values,\n\u001b[0;32m    628\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    629\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    630\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[0;32m    631\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    633\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:451\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     result_mask \u001b[38;5;241m=\u001b[39m result_mask[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m--> 451\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_cython_op(\n\u001b[0;32m    452\u001b[0m     values2d,\n\u001b[0;32m    453\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m    454\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[0;32m    455\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[0;32m    456\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    457\u001b[0m     result_mask\u001b[38;5;241m=\u001b[39mresult_mask,\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:535\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[1;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m     func(\n\u001b[0;32m    524\u001b[0m         result,\n\u001b[0;32m    525\u001b[0m         counts,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    531\u001b[0m         is_datetimelike\u001b[38;5;241m=\u001b[39mis_datetimelike,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# We support datetimelike\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomp_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatetimelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_datetimelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    544\u001b[0m     func(result, counts, values, comp_ids, min_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\groupby.pyx:540\u001b[0m, in \u001b[0;36mpandas._libs.groupby.group_add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "fdx = 'Frt Rev New '\n",
    "tnt = 'Freight Rev Old inc RFP + BXT'\n",
    "pivot1 = master_data.groupby(['Sce','Zoning','New Payweightband']).agg({fdx:'sum',tnt:'sum'}).reset_index()\n",
    "pivot1['TNT_vs_FDX'] = pivot1[fdx] - pivot1[tnt]\n",
    "pivot1['BN%'] = (pivot1['TNT_vs_FDX']*100/pivot1[tnt]).replace(np.inf, 0)\n",
    "overall_bn = round(sum(pivot1['TNT_vs_FDX'])*100/sum(pivot1[tnt]),2)\n",
    "print(\"The overall budget neutrality before running the optimization is: \"+str(overall_bn)+\"%\\n\")\n",
    "input(\"Press Enter to continue\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64b2d3",
   "metadata": {},
   "source": [
    "## Level 0 Optimization starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10243b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining custom function to store the binning logic of weightbands for different services\n",
    "def bin_strategy(row):\n",
    "    if row['Sce'] in ['IP','IE AIR']:\n",
    "        # defining bins of Pay Weight Band for services IP, IE\n",
    "        bins = [0, 0.5, 2.5, 5, 10, 20.5, 44.5, 70.5, 100, 300, 500, 1000, float('inf')]\n",
    "        labels = ['0_0.5', '0.5_2.5', '2.5_5','5_10', '10_20.5', '20.5_44.5', '44.5_70.5', '71_100', '100_300', '300_500', '500_1000','1000_0']\n",
    "    else: # defining bins of Pay Weight Band for services RE\n",
    "        bins = [0, 5, 10, 20, 30, 50, 70, 71, 100, 300, 500, 1000, float('inf')]\n",
    "        labels = ['0_5', '5_10', '10_20','20_30', '30_50', '50_70', '70_71', '71_100', '100_300', '300_500', '500_1000','1000_0']\n",
    "\n",
    "    result = pd.cut([row['New Pay Weight']],bins=bins,labels=labels)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a2530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subsetting the customer data table for relevant columns only\n",
    "## GRI INFALTION LOGIC TO BE APPLIED ON TNT REVENUE, OPTIMIZATION SHOULD RUN POST THAT ONLY\n",
    "data_columns = [\n",
    "                'Frt Rev New ', #FDX\n",
    "                'Freight Rev Old inc RFP + BXT', #TNT Revenue considered by FDX team in BN calculation in Frt&All-in sheet\n",
    "                'New Payweightband',\n",
    "                'New Pay Weight',\n",
    "                'Sce',\n",
    "                'Zoning',\n",
    "                'Shipment Number']\n",
    "\n",
    "data = master_data[data_columns] # subsetting only the required columns\n",
    "\n",
    "data['key'] = data.apply(bin_strategy,axis=1) # key column is the new weightband column basically\n",
    "\n",
    "# ordering the subsetted dataset in the following hierarchy: Service X Zone X Weightband\n",
    "data.sort_values(['Sce','Zoning','key'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3174aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the aggregated sum of both FDX & TNT at a weightband level\n",
    "discount_df = data.groupby(['Sce','Zoning','key']).agg({fdx:'sum',tnt:'sum'}).reset_index()\n",
    "\n",
    "# calculating the overall discount for the aggregated sum for different combinations of service X zone X weightbands\n",
    "discount_df['discount_weightband_level'] = ((-discount_df[fdx] + discount_df[tnt])/discount_df[fdx]).replace(np.inf, 0)\n",
    "\n",
    "# mapping the calculated discounts to the shipment level customer data/original data\n",
    "data_with_discount = pd.merge(data, discount_df[['Sce', 'Zoning', 'key','discount_weightband_level']], how='left', on=['Sce', 'Zoning', 'key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46257b6d",
   "metadata": {},
   "source": [
    "# Discount matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dde67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_matrix(customer_data,service,discount_matrix_format):\n",
    "    data1 = customer_data    \n",
    "    # pivoting the customer data in the format equivalent to the SPT discount matrix\n",
    "    data2 = pd.pivot_table(data1[data1['Sce']==service], values = 'discount_weightband_level', index=['key'], columns = 'Zoning').reset_index()\n",
    "\n",
    "    # renaming the columns to match the SPT tool discount matrix format\n",
    "    col_dict = {}\n",
    "    for col in list(data2):\n",
    "        if len(col) == 1:\n",
    "            col_dict[col] = 'Zone '+col\n",
    "\n",
    "    data2.rename(columns=col_dict, inplace=True)\n",
    "    \n",
    "    \n",
    "    if service=='RE':\n",
    "        # cleaning up the original discount matrix table from the SPT tool to make it ready for the join\n",
    "        discount_matrix_format.drop('Unnamed: 5', inplace = True, axis = 1)\n",
    "        discount_matrix_format.rename(columns={'Table B -Weight level Adjustment - % adjustment':'FromWght',\n",
    "                                           'Unnamed: 4':'ToWght'}, inplace=True)\n",
    "        discount_matrix_format.drop(index=0,inplace=True)\n",
    "        discount_matrix_format.reset_index(inplace=True,drop=True)\n",
    "        # creating the compound joining key\n",
    "        discount_matrix_format['key'] = discount_matrix_format[['FromWght','ToWght']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    elif service in ['IP','IE AIR']:\n",
    "         # cleaning up the original discount matrix table from the SPT tool to make it ready for the join\n",
    "        discount_matrix_format.drop('Unnamed: 5', inplace = True, axis = 1)\n",
    "        discount_matrix_format.rename(columns={'Table B -Weight level Adjustment - % adjustment':'FromWght',\n",
    "                                           'Unnamed: 4':'ToWght'}, inplace=True)\n",
    "        discount_matrix_format.drop(index=0,inplace=True)\n",
    "        discount_matrix_format.reset_index(inplace=True,drop=True)\n",
    "        # creating the compound joining key\n",
    "        discount_matrix_format['key'] = discount_matrix_format[['FromWght','ToWght']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    \n",
    "    # performing the join\n",
    "    discount_matrix_format_calculated = pd.merge(discount_matrix_format,data2,on = 'key', how = 'left', suffixes = (\"_x\",\"\")) # columns from my right table will retain their original names\n",
    "    # dropping duplicated columns\n",
    "    cols_to_drop = [i for i in discount_matrix_format_calculated.columns if i not in discount_matrix_format.columns]\n",
    "    discount_matrix_format_calculated.drop(cols_to_drop, axis=1,inplace=True)\n",
    "    # correcting the order of columns in synchronization with SPT tool discount matrix\n",
    "    discount_matrix_format_calculated = discount_matrix_format_calculated[discount_matrix_format.columns]\n",
    "    return discount_matrix_format_calculated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing discount matrix for IP\n",
    "discount_matrix_ip_calculated = discount_matrix(data_with_discount,'IP',discount_matrix_ip)\n",
    "discount_matrix_ip_calculated.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing discount matrix for IE\n",
    "discount_matrix_ie_air_calculated = discount_matrix(data_with_discount,'IE AIR',discount_matrix_ie_air)\n",
    "discount_matrix_ie_air_calculated.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b06f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing discount matrix for RE\n",
    "discount_matrix_re_calculated = discount_matrix(data_with_discount,'RE',discount_matrix_re)\n",
    "discount_matrix_re_calculated.fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d9adf",
   "metadata": {},
   "source": [
    "# Writing the discount figures back to the same SPT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2126b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writing_discount_matrix_to_spt_file(spt_tool_path,service,discount_matrix,sheet_name,target_cell):\n",
    "    xlsb_file = spt_tool_path\n",
    " \n",
    "    # Open the XLSB file with xlwings\n",
    "    wb = xw.Book(xlsb_file)\n",
    "\n",
    "    # Specify the sheet where you want to write data\n",
    "    sheet = wb.sheets[sheet_name]\n",
    "    \n",
    "    #Writig back the data frame\n",
    "    data_to_write = discount_matrix[discount_matrix.columns[2:-1]].fillna(0)\n",
    "\n",
    "    # Write data to the specific cell\n",
    "    sheet.range(target_cell).value = data_to_write.values\n",
    "              # ^^this is the specific cell where you want to write data\n",
    "    \n",
    "    # Save the changes to the XLSB file\n",
    "    wb.save()\n",
    "\n",
    "    print(\"Discount matrix for service \"+service+\" has been written to the corresponding sheet\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad975c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the discount matrices for all the services to the corresponding SPT sheet\n",
    "\n",
    "writing_discount_matrix_to_spt_file(spt_tool_path,'IP',discount_matrix_ip_calculated,'7. Disc-15N (IP)','G7')\n",
    "writing_discount_matrix_to_spt_file(spt_tool_path,'IE AIR',discount_matrix_ie_air_calculated,'8. Disc-48N (IE Air)','G7')\n",
    "writing_discount_matrix_to_spt_file(spt_tool_path,'RE',discount_matrix_re_calculated,'9. Disc-RE-REF','G7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa221e",
   "metadata": {},
   "source": [
    "# Implementing the business logic: Lighter weightbands should not be more expensive than the heavier weightbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7df040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to implement the copy-up logic on a single column\n",
    "\n",
    "def transform(col): # passing each column of a dataframe as a list\n",
    "    \n",
    "    result = [] #list to store the resulting column with copy-up rule applied\n",
    "    \n",
    "    # going to check for business constraint violation from the bottom up\n",
    "    for i, val in enumerate(list(col)[::-1]): # hence reversing the list of column values\n",
    "        \n",
    "        if np.isnan(val)==False: # handling non-null value situations\n",
    "            \n",
    "            if i == 0: # the last non-null value in the column will remain intact\n",
    "                result.append(val)\n",
    "                \n",
    "            elif np.isnan(result[-1])==True: # handling the situation where the comparison is between a null and a non-null value in the column\n",
    "                result.append(val)\n",
    "                \n",
    "            elif (val <= result[-1]) and (np.isnan(result[-1])==False): # keeping the current value intact if business logic is maintained\n",
    "                result.append(val)\n",
    "                \n",
    "            else: # the copy-up logic implementation in action\n",
    "                result.append(result[-1])\n",
    "        else: # handling null value situations\n",
    "            result.append(val)\n",
    "  \n",
    "    return (pd.Series(result[::-1])) # reversing the list again to restore to its original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the net rates\n",
    "\n",
    "def copy_up(rate_simulator_df, service, discount_matrix):\n",
    "    rate_simulator_df.rename(columns={'From Weight':'FromWght','To Weight':'ToWght'},inplace=True) # basic string manipulations to maintain consistency\n",
    "    \n",
    "    \n",
    "    \n",
    "    #If Fromwght more than 1000 will update towght as 1000+100\n",
    "    discount_matrix['ToWght'] = discount_matrix.apply(lambda x: pd.to_numeric(x['FromWght'])+100 if float(x['FromWght'])>=1000.0 else x['ToWght'], axis=1)\n",
    "    rate_simulator_df['ToWght'] = rate_simulator_df.apply(lambda x: pd.to_numeric(x['FromWght'])+100 if float(x['FromWght'])>=999.5001 else x['ToWght'], axis=1)\n",
    "\n",
    "    # mapping the discount matrix at the corresponding rows of list rate table\n",
    "    #B.FromWght=71  ,A.FromWght=70.5001\n",
    "    list_rates_with_discounts_in_rows = sqldf('''\n",
    "                                            SELECT A.*,B.*,\n",
    "                                            CASE\n",
    "                                            WHEN CAST(A.FromWght AS FLOAT) > 70.5 THEN ROUND(CAST(A.FromWght AS FLOAT))\n",
    "                                            WHEN CAST(A.FromWght AS FLOAT)<70.5 THEN CAST(A.FromWght AS FLOAT)\n",
    "                                            END AS FromWght_Round\n",
    "                                            FROM rate_simulator_df A\n",
    "                                            LEFT JOIN discount_matrix B\n",
    "                                            ON CAST(FromWght_Round AS FLOAT) >= CAST(B.FromWght AS FLOAT) \n",
    "                                            AND CAST(A.ToWght AS FLOAT) <= CAST(B.ToWght AS FLOAT)\n",
    "                                            ''')\n",
    "    \n",
    "    discounted_list_rates = rate_simulator_df.copy()\n",
    "\n",
    "    # applying the discounts to respective columns\n",
    "    for i in list_rates_with_discounts_in_rows.columns:\n",
    "        for j in list_rates_with_discounts_in_rows.columns:\n",
    "            if 'Zone '+i==j:\n",
    "                discounted_list_rates[i] = list_rates_with_discounts_in_rows[i]*(1+list_rates_with_discounts_in_rows[j])\n",
    "                \n",
    "    #Copy_up logic will apply if ToWght less than 71          \n",
    "    discounted_list_rates_less70_5001=discounted_list_rates[pd.to_numeric(discounted_list_rates['ToWght'])<71.00]\n",
    "    discounted_list_rates_greater70_5001=discounted_list_rates[pd.to_numeric(discounted_list_rates['ToWght'])>=71.00]\n",
    "    \n",
    "    \n",
    "    net_rates = discounted_list_rates_less70_5001.copy()\n",
    "\n",
    "    # choosing only the zone columns\n",
    "    net_rates_cols = [i for i in net_rates.columns if len(i)==1]\n",
    "\n",
    "    # passing each of the zone columns to the transform functions\n",
    "    for i in net_rates_cols:\n",
    "        net_rates[i] = transform(discounted_list_rates_less70_5001[i])\n",
    "        \n",
    "    discounted_list_rates = pd.concat([net_rates,discounted_list_rates_greater70_5001])\n",
    "\n",
    "    return discounted_list_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f863b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rates_ip = copy_up(rate_simulator_ip,'IP',discount_matrix_ip_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rates_ie_air = copy_up(rate_simulator_ie_air,'IE AIR',discount_matrix_ie_air_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_rates_re = copy_up(rate_simulator_re,'RE',discount_matrix_re_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c63b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_rates_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writing_rate_simulator_to_spt_file(spt_tool_path,service,rate_simulator,sheet_name,target_cell):\n",
    "    xlsb_file = spt_tool_path\n",
    " \n",
    "    # Open the XLSB file with xlwings\n",
    "    wb = xw.Book(xlsb_file)\n",
    "\n",
    "    # Specify the sheet where you want to write data\n",
    "    sheet = wb.sheets[sheet_name]\n",
    "    \n",
    "    #Writig back the data frame\n",
    "    data_to_write = rate_simulator[rate_simulator.columns[2:-1]].fillna(0)\n",
    "\n",
    "    # Write data to the specific cell\n",
    "    sheet.range(target_cell).value = data_to_write.values\n",
    "              # ^^this is the specific cell where you want to write data\n",
    "    \n",
    "    # Save the changes to the XLSB file\n",
    "    wb.save()\n",
    "\n",
    "    print(\"Rate simulator for service \"+service+\" has been written to the corresponding sheet\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_rate_simulator_to_spt_file(spt_tool_path,'IP',net_rates_ip,[x for x in all_sheetname if x.startswith('Rate Simulator IP')][0],'AA5')\n",
    "writing_rate_simulator_to_spt_file(spt_tool_path,'IE AIR',net_rates_ie_air,[x for x in all_sheetname if x.startswith('Rate Simulator IE')][0],'AA5')\n",
    "writing_rate_simulator_to_spt_file(spt_tool_path,'RE',net_rates_re,[x for x in all_sheetname if x.startswith('Rate Simulator RE-REF')][0],'AF5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c44a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "master_test=master_data.copy()\n",
    "\n",
    "master_test['Shipment Date'] = pd.to_numeric(master_data['Shipment Date'], errors='coerce')\n",
    "\n",
    "# Function to convert Excel serial date number to date format\n",
    "convert_to_date = lambda excel_date: (datetime.datetime(1899, 12, 31) + datetime.timedelta(days=excel_date - 1)).strftime('%m/%d/%Y') if excel_date > 0 else None\n",
    " \n",
    "# Applying the lambda function to the 'excel_date' column\n",
    "master_test['Shipment Date'] = master_test['Shipment Date'].apply(convert_to_date)\n",
    "\n",
    "master_test[(master_test['Shipment Date']>'10/03/2022') &  (master_test['Shipment Date']<'12/03/2022')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee3c05",
   "metadata": {},
   "source": [
    "# END OF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03849a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['key'] =   np.where((data['New Pay Weight']<=0.5)&(data['Sce'].isin(['IP','IE AIR'])), '0_0.5',\n",
    "#                 np.where((data['New Pay Weight']<=2.5)&(data['Sce'].isin(['IP','IE AIR'])), '0.5_2.5',\n",
    "#                 np.where((data['New Pay Weight']<=5)&(data['Sce'].isin(['IP','IE AIR'])), '2.5_5',\n",
    "#                 np.where((data['New Pay Weight']<=10)&(data['Sce'].isin(['IP','IE AIR'])), '5_10',\n",
    "#                 np.where((data['New Pay Weight']<=20.5)&(data['Sce'].isin(['IP','IE AIR'])), '10_20.5',\n",
    "#                 np.where((data['New Pay Weight']<=44.5)&(data['Sce'].isin(['IP','IE AIR'])), '20.5_44.5',\n",
    "#                 np.where((data['New Pay Weight']<=70.5)&(data['Sce'].isin(['IP','IE AIR'])), '44.5_70.5',\n",
    "#                 np.where((data['New Pay Weight']<=100)&(data['Sce'].isin(['IP','IE AIR'])), '71_100',\n",
    "#                 np.where((data['New Pay Weight']<=300)&(data['Sce'].isin(['IP','IE AIR'])), '100_300',\n",
    "#                 np.where((data['New Pay Weight']<=500)&(data['Sce'].isin(['IP','IE AIR'])), '300_500',\n",
    "#                 np.where((data['New Pay Weight']<=1000)&(data['Sce'].isin(['IP','IE AIR'])), '500_1000',\n",
    "#                 np.where((data['New Pay Weight']>1000)&(data['Sce'].isin(['IP','IE AIR'])), '1000_0',\n",
    "#                 np.where((data['New Pay Weight']<=5)&(data['Sce']=='RE'), '0_5',\n",
    "#                 np.where((data['New Pay Weight']<=10)&(data['Sce']=='RE'), '5_10',\n",
    "#                 np.where((data['New Pay Weight']<=20)&(data['Sce']=='RE'), '10_20',\n",
    "#                 np.where((data['New Pay Weight']<=30)&(data['Sce']=='RE'), '20_30',\n",
    "#                 np.where((data['New Pay Weight']<=50)&(data['Sce']=='RE'), '30_50',\n",
    "#                 np.where((data['New Pay Weight']<=70)&(data['Sce']=='RE'), '50_70',\n",
    "#                 np.where((data['New Pay Weight']<=71)&(data['Sce']=='RE'), '70_71',\n",
    "#                 np.where((data['New Pay Weight']<=100)&(data['Sce']=='RE'), '71_100',\n",
    "#                 np.where((data['New Pay Weight']<=300)&(data['Sce']=='RE'), '100_300',\n",
    "#                 np.where((data['New Pay Weight']<=500)&(data['Sce']=='RE'), '300_500',\n",
    "#                 np.where((data['New Pay Weight']<=1000)&(data['Sce']=='RE'), '500_1000',\n",
    "#                 np.where((data['New Pay Weight']>1000)&(data['Sce']=='RE'), '1000_0','NaN'))))))))))))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bins = {\n",
    "#     'IP': [0, 0.5, 2.5, 5, 10, 20.5, 44.5, 70.5, 100, 300, 500, 1000, float('inf')],\n",
    "#     'IE AIR': [0, 0.5, 2.5, 5, 10, 20.5, 44.5, 70.5, 100, 300, 500, 1000, float('inf')],\n",
    "#     'RE': [0, 5, 10, 20, 30, 50, 70, 71, 100, 300, 500, 1000, float('inf')]\n",
    "# }\n",
    "\n",
    "# # Use pandas cut to create the 'Binned_Age' column\n",
    "# data['key'] = data.apply(lambda row: pd.cut([row['New Pay Weight']], bins[row['Sce']], labels=False), axis=1)\n",
    "\n",
    "# # Map the labels to the corresponding bins\n",
    "# bin_labels = {\n",
    "#     'IP': ['0_0.5', '0.5_2.5', '2.5_5','5_10', '10_20.5', '20.5_44.5', '44.5_70.5', '71_100', '100_300', '300_500', '500_1000','1000_0'],\n",
    "#     'IE AIR': ['0_0.5', '0.5_2.5', '2.5_5','5_10', '10_20.5', '20.5_44.5', '44.5_70.5', '71_100', '100_300', '300_500', '500_1000','1000_0'],\n",
    "#     'RE': ['0_5', '5_10', '10_20','20_30', '30_50', '50_70', '70_71', '71_100', '100_300', '300_500', '500_1000','1000_0']\n",
    "# }\n",
    "# data['key'] = data.apply(lambda row: bin_labels[row['Sce']][row['New Pay Weight']], axis=1)\n",
    "\n",
    "# if data['Sce'].isin(['IP','IE AIR']):\n",
    "#     data['key'] = pd.cut(x=data[data['Sce'].isin(['IP','IE AIR'])]['New Pay Weight'], \n",
    "#                          bins=[0, 0.5, 2.5, 5, 10, 20.5, 44.5, 70.5, 100, 300, 500, 1000, 10000],\n",
    "#                          labels=['0_0.5', '0.5_2.5', '2.5_5','5_10', '10_20.5', '20.5_44.5', '44.5_70.5', '71_100', '100_300', '300_500', '500_1000','1000_0'])\n",
    "\n",
    "# data[data['Sce']=='RE']['key'] = pd.cut(x=data[data['Sce']=='RE']['New Pay Weight'], \n",
    "#                      bins=[0, 5, 10, 20, 30, 50, 70, 71, 100, 300, 500, 1000, 10000],\n",
    "#                      labels=['0_5', '5_10', '10_20','20_30', '30_50', '50_70', '70_71', '71_100', '100_300', '300_500', '500_1000','1000_0'])\n",
    "# data[data['Sce'].isin(['IP','IE AIR'])]['key'] = data[data['Sce'].isin(['IP','IE AIR'])].assign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11446f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         data['key'] =   np.where((data['New Pay Weight']<=0.5)&(data['Sce'].isin(['IP','IE AIR'])), '0_0.5',\n",
    "#                         np.where((data['New Pay Weight']<=2.5)&(data['Sce'].isin(['IP','IE AIR']), '0.5_2.5',\n",
    "#                         np.where((data['New Pay Weight']<=5)&(data['Sce'].isin(['IP','IE AIR']), '2.5_5',\n",
    "#                         np.where((data['New Pay Weight']<=10)&(data['Sce'].isin(['IP','IE AIR']), '5_10',\n",
    "#                         np.where((data['New Pay Weight']<=20.5)&(data['Sce'].isin(['IP','IE AIR']), '10_20.5',\n",
    "#                         np.where((data['New Pay Weight']<=44.5)&(data['Sce'].isin(['IP','IE AIR']), '20.5_44.5',\n",
    "#                         np.where((data['New Pay Weight']<=70.5)&(data['Sce'].isin(['IP','IE AIR']), '44.5_70.5',\n",
    "#                         np.where((data['New Pay Weight']<=100)&(data['Sce'].isin(['IP','IE AIR']), '71_100',\n",
    "#                         np.where((data['New Pay Weight']<=300)&(data['Sce'].isin(['IP','IE AIR']), '100_300',\n",
    "#                         np.where((data['New Pay Weight']<=500)&(data['Sce'].isin(['IP','IE AIR']), '300_500',\n",
    "#                         np.where((data['New Pay Weight']<=1000)&(data['Sce'].isin(['IP','IE AIR']), '500_1000',\n",
    "#         data['key'] =   np.where(data['New Pay Weight']<=5, '0_5',\n",
    "#                         np.where(data['New Pay Weight']<=10, '5_10',\n",
    "#                         np.where(data['New Pay Weight']<=20, '10_20',\n",
    "#                         np.where(data['New Pay Weight']<=30, '20_30',\n",
    "#                         np.where(data['New Pay Weight']<=50, '30_50',\n",
    "#                         np.where(data['New Pay Weight']<=70, '50_70',\n",
    "#                         np.where(data['New Pay Weight']<=71, '70_71',\n",
    "#                         np.where(data['New Pay Weight']<=100, '71_100',\n",
    "#                         np.where(data['New Pay Weight']<=300, '100_300',\n",
    "#                         np.where(data['New Pay Weight']<=500, '300_500',\n",
    "#                         np.where(data['New Pay Weight']<=1000, '500_1000','1000_0')))))))))))\n",
    "    \n",
    "# max_val = data['New Pay Weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd79231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivoting the customer data in the format equivalent to the SPT discount matrix\n",
    "# data2 = pd.pivot_table(data1[data1['Sce']=='IE AIR'], values = 'discount_weightband_level', index=['New Payweightband'], columns = 'Zoning').reset_index()\n",
    "# # creating required columns for joining with SPT discount matrix\n",
    "# data2['FromWght'] = ['0',\t'0.5',\t'2.5',\t'5',\t'10',\t'20.5',\t'44.5',\t'71',\t'100',\t'300']\n",
    "# data2['ToWght'] =   ['0.5',\t'2.5',\t'5',\t'10',\t'20.5',\t'44.5',\t'70.5',\t'100',\t'300',\t'500']\n",
    "# # creating the compound joining key\n",
    "# data2['key'] = data2[['FromWght','ToWght']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# # renaming the columns to match the SPT tool discount matrix format\n",
    "# col_dict = {}\n",
    "# for col in list(data2):\n",
    "#     if len(col) == 1:\n",
    "#         col_dict[col] = 'Zone '+col\n",
    "        \n",
    "# data2.rename(columns=col_dict, inplace=True)\n",
    "# data2.head()\n",
    "\n",
    "# # cleaning up the original discount matrix table from the SPT tool to make it ready for the join\n",
    "# discount_matrix_ip.drop('Unnamed: 5', inplace = True, axis = 1)\n",
    "# discount_matrix_ip.rename(columns={'Table B -Weight level Adjustment - % adjustment':'FromWght',\n",
    "#                                    'Unnamed: 4':'ToWght'}, inplace=True)\n",
    "# discount_matrix_ip.drop(index=0,inplace=True)\n",
    "# discount_matrix_ip.reset_index(inplace=True,drop=True)\n",
    "# # creating the compound joining key\n",
    "# discount_matrix_ip['key'] = discount_matrix_ip[['FromWght','ToWght']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "# discount_matrix_ip.head()\n",
    "\n",
    "# # performing the join\n",
    "# discount_matrix_ip_calculated2 = pd.merge(discount_matrix_ip,data2,on = 'key', how = 'left', suffixes = (\"_x\",\"\")) # columns from my right table will retain their original names\n",
    "# # dropping duplicated columns\n",
    "# cols_to_drop = [i for i in discount_matrix_ip_calculated2.columns if i not in discount_matrix_ip.columns]\n",
    "# discount_matrix_ip_calculated2.drop(cols_to_drop, axis=1,inplace=True)\n",
    "# # correcting the order of columns in synchronization with SPT tool discount matrix\n",
    "# discount_matrix_ip_calculated2 = discount_matrix_ip_calculated2[discount_matrix_ip.columns]\n",
    "# discount_matrix_ip_calculated2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index,row in data.iterrows():\n",
    "#     if (row[0]!=0)&(row[1]!=0):\n",
    "#         data.loc[index,'discount%'] = (-row[1] + row[0])/row[1]\n",
    "#     else:\n",
    "#         data.loc[index,'discount%'] = float(\"Nan\")\n",
    "\n",
    "# data['bn_level1%'] = (data['Frt Rev New ']*(1 - data['discount%']/100) - data['Freight Revenue'])/data['Freight Revenue']*100\n",
    "\n",
    "# mean_discount = data.groupby(['Sce','Zoning','New Payweightband'], as_index=False, sort=False).mean()\n",
    "\n",
    "# discount_matrix_ip = pd.pivot_table(mean_discount[mean_discount['Service']=='Express'], values = 'discount%', index=['New Payweightband'], columns = 'Zoning').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0012add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dups = data.groupby('Shipment Number').filter(lambda x: len(x) > 1)\n",
    "# dups_list = list(set(dups['Shipment Number']))\n",
    "\n",
    "# data_unique = data[~data['Shipment Number'].isin(dups_list)]\n",
    "# len(data_unique)\n",
    "\n",
    "# data['bn_level0%'] = (data['Frt Rev New '] - data['Freight Revenue'])/data['Freight Revenue']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9eea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_discount[(mean_discount['Service']=='Economy')&(mean_discount['Zoning']=='A')&\n",
    "#               (mean_discount['New Pay Weight']>=0.5)&(mean_discount['New Pay Weight']<=2.5)\n",
    "#              ]['discount%'].values[0]\n",
    "\n",
    "# for index,row in discount_matrix_ip.iterrows():\n",
    "#     discount_matrix_ip.loc[index,'Zone A'] = mean_discount[(mean_discount['Service']=='Express')&(mean_discount['Zoning']=='A')&\n",
    "#                                                            (mean_discount['New Pay Weight']>=row[0])&(mean_discount['New Pay Weight']<=row[1])\n",
    "#                                                           ]['discount%'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbdef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Shipment Number'].nunique()\n",
    "# disc = data.groupby('Shipment Number').filter(lambda x: len(x) > 1)\n",
    "# list(set(disc['Shipment Number']))\n",
    "# data[data['Shipment Number']==614872888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063522da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace 'your_file.xlsx' with the path to your Excel file\n",
    "# file_path = 'C:\\\\Users\\\\rishav.ganguly\\\\Downloads\\\\FedEx\\\\FDX SPT - 2024 SAS v4.3 - GB Export_GBP_Accenture (2)\\\\FDX SPT - 2024 SAS v4.3 - GB Export_GBP_Accenture.xlsb',\n",
    "# # Replace 'Sheet1' with the name of the Excel tab\n",
    "# sheet_name = '2. Add. Info'\n",
    "\n",
    "# # Define the ranges for each table in the Excel tab\n",
    "# # Each range should be specified as 'A1:B3', 'C5:E10', etc., where 'A1', 'B3', etc., define the table boundaries\n",
    "# table_ranges = {\n",
    "#     'CCF': 'O:Q',  # Replace with your desired range\n",
    "#     'Clearance Fee': 'K:M',  # Replace with your desired range\n",
    "#     # Add more tables as needed\n",
    "# }\n",
    "\n",
    "# # Create an empty dictionary to store the tables\n",
    "# tables = {}\n",
    "\n",
    "# # Read each specified range into a DataFrame and store it in the 'tables' dictionary\n",
    "# with pd.ExcelFile(file_path) as xls:\n",
    "#     for table_name, range_str in table_ranges.items():\n",
    "#         df = pd.read_excel(xls, engine='pyxlsb', sheet_name=sheet_name, header=0, usecols=range_str)\n",
    "#         tables[table_name] = df\n",
    "\n",
    "# # Now, 'tables' is a dictionary containing separate DataFrames for each specified table in the Excel tab\n",
    "# # You can access each table using its name as the key\n",
    "# # For example, to access 'Table1':\n",
    "# ccf = tables['CCF']\n",
    "# ccf\n",
    "\n",
    "# # Repeat the above process for each table you want to extract from the Excel tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18545ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #import all the libraries\n",
    "# from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "# from office365.sharepoint.client_context import ClientContext\n",
    "# from office365.sharepoint.files.file import File \n",
    "# import io\n",
    "# # # import pandas as pd\n",
    "\n",
    "# # # #target url taken from sharepoint and credentials\n",
    "# url = 'https://ts.accenture.com/:x:/s/FedEx55/EWyUT6mK60BKnneUTCACVg0B2EgdeiP7XbJnWsKy3LLWxQ?e=gMVnYL'\n",
    "# username = 'rishav.ganguly@accenture.com'\n",
    "# password = 'Puku@0707wb'\n",
    "\n",
    "# ctx_auth = AuthenticationContext(url)\n",
    "# if ctx_auth.acquire_token_for_user(username, password):\n",
    "#   ctx = ClientContext(url, ctx_auth)\n",
    "#   web = ctx.web\n",
    "#   ctx.load(web)\n",
    "#   ctx.execute_query()\n",
    "#   print(\"Authentication successful\")\n",
    "\n",
    "# response = File.open_binary(ctx, url)\n",
    "\n",
    "# # #save data to BytesIO stream\n",
    "# bytes_file_obj = io.BytesIO()\n",
    "# bytes_file_obj.write(response.content)\n",
    "# bytes_file_obj.seek(0) #set file object to start\n",
    "\n",
    "# # # #read excel file and each sheet into pandas dataframe \n",
    "# # df = pd.read_excel(bytes_file_obj, engine='pyxlsb', sheet_name = 'Parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78301b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# text = 'c 2.51-5kg'\n",
    "\n",
    "# # Define a regular expression pattern to match decimal numbers\n",
    "# pattern = r'(\\d+\\.\\d+)'\n",
    "\n",
    "# # Use re.findall to find all matching numbers in the text\n",
    "# numeric_list = re.findall(pattern, text)\n",
    "\n",
    "# # Convert the list of matched strings to a list of floats\n",
    "# numeric_list = [round(float(num),1) for num in numeric_list]\n",
    "\n",
    "# print(numeric_list)\n",
    "\n",
    "# #############################################################################################################\n",
    "# import re\n",
    "\n",
    "# def extract_numeric_range(text):\n",
    "#     # Define a regular expression pattern to match decimal numbers\n",
    "#     pattern = r'(\\d+\\.\\d+)-(\\d+\\.\\d+)'\n",
    "\n",
    "#     # Use re.search to find the first match in the text\n",
    "#     match = re.search(pattern, text)\n",
    "\n",
    "#     # Check if a match was found\n",
    "#     if match:\n",
    "#         start = float(match.group(1))\n",
    "#         end = float(match.group(2))\n",
    "#         # Round to the nearest tenth\n",
    "#         return [round(start, 1), round(end, 1)]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Test with your examples\n",
    "# examples = [\n",
    "#     \"b 0.51-2.5kg\",\n",
    "#     \"c 2.51-5kg\",\n",
    "#     \"f 20.51-44.5kg\",\n",
    "#     \"g 44.51-70.5kg\"\n",
    "# ]\n",
    "\n",
    "# for example in examples:\n",
    "#     numeric_range = extract_numeric_range(example)\n",
    "#     if numeric_range is not None:\n",
    "#         print(f\"Input text: '{example}'\")\n",
    "#         print(f\"Output numeric list: {numeric_range}\")\n",
    "#         print()\n",
    "#     else:\n",
    "#         print(f\"No numeric range found in '{example}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
